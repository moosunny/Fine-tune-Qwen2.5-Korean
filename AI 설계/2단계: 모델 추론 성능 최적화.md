<h3>GCP VM 구성</h3>
<ul>
<li>
<p>CPU: Intel Cascade Lake(vCPU 8개, 메모리 32GB)</p>
<ul>
<li>GPU: NVIDIA L4 1ea</li>
<li>부팅 디스크 용량: 200GB(Ubuntu 20.04 x86/64)</li>
</ul>

<h3>모델 GPU VRAM 사용량 측정</h3>
<ul>
<li>Nvidia driver 설치 후 <code>watch -n 1 nvidia-smi</code> 로 초단위 GPU 사용량 모니터링 수행</li>
</ul>

<h3>Model Inference 예시</h3>




<pre><code class="language-python">
from vllm import LLM, SamplingParams
import torch
from pydantic import BaseModel

# 모델 위치
gemma_3_4b_it =  &quot;/home/mmm060400/KTB/models/gemma-3-4b-it/&quot;
# 모델 구동
# 1. 모델 디렉토리 경로
# 2. GPU 메모리 최대 사용치
# 3. GPU 병렬 개수
# llm = LLM(model=model_path, gpu_memory_utilization=0.95, tensor_parallel_size=1, max_model_len = 2048)
llm = LLM(model=gemma_3_4b_it, 
          gpu_memory_utilization=0.95, 
          dtype= torch.bfloat16,
          tensor_parallel_size=1, 
          max_model_len = 2048,)
         
          
          
from datetime import datetime
now = datetime.now().strftime('%Y-%m-%d')
lang = dataset[&quot;train&quot;][0][&quot;lang&quot;] # github code review 데이터

prompt = f&quot;&quot;&quot;
    You are an assistant that writes structured, markdown-based technical &quot;Today I Learned (TIL)&quot; reports in Korean, based on Git code diffs. 

    Your output must be a **valid JSON** object with three fields:

    1. &quot;title&quot;: A concise and formal Korean title for the TIL. It should summarize the core change and include the date in the format: &quot;{now} TIL - ...&quot;.
    2. &quot;keywords&quot;: A list of 1 to 3 key Korean technical concepts (strings) that summarize the content. Each keyword should be one or two words, and relevant to the code changes.
    3. &quot;content&quot;: The full TIL content in **Korean**, using Markdown. Structure it like this:
    - Start with a Markdown title: `# {now} TIL`
    - Summarize the key changes in a **formal, bullet-point list**
    - Include relevant **code snippets** below each point
    - End each bullet point with a period.
    - Optionally, explain maintainability, accessibility, or scalability concerns if they are relevant.

    Constraints:
    - Only return a JSON object with &quot;title&quot;, &quot;keywords&quot;, and &quot;content&quot; fields.
    - Do not include any extra commentary or preamble.
    - Avoid phrases like &quot;I learned&quot;, &quot;I think&quot;, or any personal expressions.

    Now, please generate the response based on the following code diff:
</code></pre>

## LLM 서빙 도구 도입 배경
최근 LLM 기반 서비스가 다양화되면서, 단순한 로컬 테스트를 넘어서 고속 응답, 동시 사용자 처리, 리소스 최적화, API 제공 같은 실제 운영 환경에 맞는 서빙 전략이 필수로 떠오르고 있기 때문에, 다음과 같은 상황에서는 별도의 서빙 엔진이 필요함:

✅ 초당 수십~수백 건의 요청을 처리해야 하는 경우

✅ Streaming 응답이 요구되는 경우

✅ GPU 메모리와 성능을 최대한 활용해야 하는 경우

✅ **다양한 양자화 모델 또는 저사양 환경(CPU)**에서 추론이 필요한 경우

이러한 요구를 충족하기 위해 대표적으로 사용되는 도구가 바로 vLLM과 llama-cpp-python이며, 두 도구는 각각 서버 최적화와 로컬 경량화에 특화되어 있어, 용도에 따라 선택하는 것이 중요함





## 🔍 vLLM vs llama-cpp-python: 서빙 환경 특징

| 비교 항목        | vLLM                                         | llama-cpp-python                                 |
|------------------|----------------------------------------------|--------------------------------------------------|
| 지원 모델 형식   | Hugging Face (FP16, bfloat16)               | GGUF (INT4~8, float16)                           |
| 추론 환경        | GPU (CUDA 필수)                             | CPU 또는 GPU                                     |
| 장점             | - 대규모 모델 추론 최적화<br>- OpenAI API 호환<br>- PagedAttention + Streaming 지원 | - 초경량 모델<br>- CPU 동작 가능<br>- 빠른 초기 응답 |
| 단점             | - 높은 VRAM 사용<br>- 로딩 시간 김<br>- 작은 GPU에는 부적합 | - 품질 저하 가능<br>- 서버 최적화 미흡          |
| 서빙 구조        | `vllm.serve()` + FastAPI proxy 가능         | 내장 서버 제공 또는 FastAPI 직접 구성            |
| 적합한 활용      | 실시간 응답, 고품질 생성 태스크             | 저자원 환경, 코드 해석, 단순 추론                |
| 예시 활용        | TIL 생성, 문서 요약, OpenAI API 대체        | CLI 툴, 코드 요약기, 임베딩 추출 등              |

---

## vLLM 추론 속도 비교

동일한 질문에 대한 응답 속도 비교

| 모델 | speed input | speed output | VRAM 용량 | 답변 품질 | 상용화 가능 여부 |
| --- | --- | --- | --- | --- | --- |
| Gemma3-4b-it | 0.86 toks/s | 28.21 toks/s | 21000MiB / 23034MiB | 대체적으로 만족스러움 | 가능 |
| Gemma3-1b-it | 1.19 toks/s | 86.35 toks/s | 21902MiB / 23034MiB | 매우 낮음, 거짓말 다수, 답변 형식 부족 | 가능 |
| llama3.1-8b-it | 0.17 toks/s | 16.07 toks/s | 22084MiB / 23034MiB | 만족스러운 답변 수준 | 불가능 |
| deepseek-r1-qwen-7b | 1.07 toks/s | 16.78 toks/s | 22038MiB / 23034MiB | 답변 길이가 짧고 성의 없음 | 가능 |
| Qwen2.5-7b-it-1m | 0.30 toks/s | 16.85 toks/s | 22036MiB / 23034MiB | 성의 부족, 형식화되지 않음 | 가능 |

---

## llama-cpp-python 추론 속도 비교

| 모델 | speed input | speed output | VRAM 용량 | 답변 품질 |
| --- | --- | --- | --- | --- |
| Qwen2.5-coder-3b.gguf (6bit) | 14.72 ms/token | 67.95 tokens/s | 5938MiB / 23034MiB | 코드 해석 우수, TIL 작성은 형식 부족 |
| Gemma3-4b-it.gguf (6bit) | 14.76 ms/token | 67.76 tokens/s | 5970MiB / 23034MiB | 양자화 후 한국어 품질 저하 발생 |

---

## llama-cpp-python VS vLLM

| 항목 | vLLM | llama-cpp-python |
| --- | --- | --- |
| 제공 모델 | Hugging Face 기반 FP16 모델 | GGUF 형식의 경량화 모델 |
| 실행 환경 | CUDA 필수 (GPU) | CPU 또는 GPU |
| 지원 기능 | 비동기, KV 캐시, Tensor Parallel 지원 | 로우 레벨 추론, 다양한 양자화 지원 |
| 특징 | 대규모 LLM 서버 추론 최적화 | 초경량 추론 최적화 |
| 장점 | - GPU 활용 최적화 (`PagedAttention`)<br>- 다중 요청 처리에 강함<br>- `vllm.serve()`로 OpenAI 호환 API 제공<br>- FastAPI 연동 쉬움 | - 매우 낮은 메모리 점유 (INT4, INT8)<br>- CPU 기반 가능<br>- 가벼운 모델(1~4B) 추론 속도 빠름<br>- embedding / streaming 지원 |
| 단점 | - VRAM 많이 사용<br>- 모델 로딩 무겁고 느림<br>- 작은 GPU에 부적합 | - 출력 품질 낮을 수 있음<br>- GPU 병렬/서버 최적화 부족<br>- 대규모 트래픽 대응 어려움 |
| FastAPI 연동 | `vllm.serve()` 후 FastAPI proxy 구성 | `llama_cpp.server` 내장 or FastAPI 직접 구성 |

---
## vLLM 추론 모델 성능 비교

| 모델        | precision | recall | F1 score | speed input     | speed output    | VRAM 용량  |
|-------------|-----------|--------|----------|------------------|------------------|------------|
| phi-4       | 0.5786    | 0.6900 | 0.6294   | 3.04 ms/token    | 42.04 tokens/s   | 19711MiB   |
| Mistral-7B  | 0.9435    | 0.9776 | 0.9602   | 7.43 ms/token    | 17.23 tokens/s   | 19886MiB   |

---

<h3>LLM 한국어 성능 측정</h3>
<p>SK Telecom에서 만든 벤치마크 데이터셋 기준 LLM 한국어 능력 평가 진행(f1-score 기준)

<code>lm-evaluation-harness</code> : 다양한 벤치마크에서 언어 모델의 성능을 자동으로 평가할 수 있는 프레임워크</p>
<ul>
<li><strong>KB-BoolQ</strong>: 문단에서 주어진 질문에 대해 참 또는 거짓을 판단</li>
<li><strong>KB-COPA</strong>: 주어진 상황에 가장 적합한 결과를 두 가지 대안 중에서 선택</li>
<li><strong>KB-WiC</strong>: 같은 단어가 서로 다른 문맥에서 같은 의미로 사용되는지 판단</li>
<li><strong>KB-HellaSwag</strong>: 상황 설명에 맞는 다음 전개를 네 가지 선택지 중에서 고름</li>
<li><strong>KB-SentiNeg</strong>: 문장의 긍정 또는 부정을 정확히 분류하는 능력을 평가</li>
</ul>

모델 | kobest_boolq | kobest_copa | kobest_hellaswag | kobest_sentineg | kobest_wic
-- | -- | -- | -- | -- | --
Mistral-7B-Instruct-v0.2 | 0.8145 | 0.5832 | 0.4860 | 0.5239 | 0.3438
Llama-3.1-8B-Instruct | 0.5869 | 0.6224 | 0.4238 | 0.8246 | 0.3280
Qwen2.5-7B-Instruct-1M | 0.5279 | 0.6963 | 0.5540 | 0.7237 | 0.3280
kanana-nano-2.1b-instruct | 0.6561 | 0.7896 | 0.4854 | 0.9748 | 0.3280
EXAONE-Deep-7.8B | 0.6728 | 0.6315 | 0.3786 | 0.3511 | 0.4023
Gemma-2-9B-Instruct-4Bit-GPTQ | 0.8105 | 0.7397 | 0.4423 | 0.7589 | 0.3297
Gemma-2-9b-it | 0.9152 | 0.7485 | 0.4520 | 0.8699 | 0.3276
Gemma-3-12b-it | 0.9160 | 0.8197 | 0.4658 | 0.9267 | 0.3976


<p>한국어 능력 평가 지표에서 가장 우수한 모델인 <code>Gemma-3-12b-it</code> 모델을 1차 MVP 개발 모델로 선정</p>
<ul>
<li>12b 파라미터를 갖고 있는 모델의 경우 24GB VRAM이 감당할 수 없기 때문에 GPTQ 양자화 모델 활용</li>
<li>GPTQ 양자화는 한국어 성능 저하가 두드러지지 않고 사후 양자화 방법임을 고려했을 때, 활용이 용이함

---



## vLLM 기반 LLM 서빙 최적화 계획

현재 구현 중인 **TIL 자동 작성 시스템**은 커밋 메시지, 코드 변경 이력 등 복합 정보를 입력으로 받아 마크다운 형식의 텍스트를 생성하는 **생성형 태스크**

이 시스템은 다음과 같은 요구 조건을 요구:
- 연속된 대화 처리
- 다양한 길이의 입력 텍스트
- 스트리밍 응답
- 짧은 대기시간
- 일정 수준 이상의 응답 품질

따라서 모델 서빙 조건으로는:
- **짧은 Latency**
- **높은 Throughput**
- **Streaming 응답 지원**
- **고품질 응답 유지**

이러한 요건에 가장 부합하는 추론 엔진은 **vLLM**으로 판단


<hr>
<h2>적용예정 최적화 기법</h2>
<h3>1. <strong>PagedAttention 기반의 시퀀스 병렬화 (Continuous Batching)</strong></h3>
<ul>
<li>vLLM은 자체 구현한 <code>PagedAttention</code> 알고리즘을 통해, 서로 다른 길이의 시퀀스들을 하나의 배치(batch)로 처리</li>
<li>기존 Transformers 라이브러리에서는 padding이나 비효율적인 메모리 할당으로 인해 처리량 손실이 있었으나, vLLM은 GPU 메모리를 <strong>가상 메모리 페이지처럼 슬라이스</strong>해 최적화된 키/밸류 캐시 관리</li>
</ul>
<p>➡️ <strong>결과</strong>: 대기 시간 감소 + 처리량 증가 (especially for concurrent users)</p>
<hr>
<h3>2. <strong>비동기 FastAPI 연동 (StreamingResponse + asyncio)</strong></h3>
<ul>
<li>사용자 요청은 FastAPI 기반 프론트 서버로 받아, 내부적으로 vLLM 서버(<code>OpenAI API 호환</code>)와 비동기적으로 통신하도록 구성</li>
<li><code>stream=True</code> 모드 사용 시 응답이 생성되는 대로 사용자에게 실시간으로 반환되며, 체감 응답속도가 향상</li>
</ul>
<p><strong>결과</strong>: 사용자 응답 체감속도 ↑, UX 향상</p>
<hr>
<h3>3. <strong>L4 GPU 최적 메모리 분할 및 모델 선택</strong></h3>
<ul>
<li><code>google/gemma3-4b-it</code> 또는 <code>Mistral-7B-Instruct</code> 계열 모델을 기준으로, <code>fp16</code> 정밀도 및 <code>tensor_parallel_size=1</code>로 최적 설정</li>
<li>VRAM 24GB를 기준으로 최대 batch size ≒ 8~12 요청까지 동시 처리 가능(예상)</li>
</ul>
<p>➡️ <strong>결과</strong>: 자원 효율성을 유지하면서도 고품질 응답 생성 가능</p>
<hr>
<h3><strong>4. RAG 효율 최적화 (Embedding + 유사도 검색 속도 개선)</strong></h3>
<ul>
<li><strong>목적</strong>: 유사 질문 검색 정확도와 속도 향상</li>
<li><strong>기법</strong>:
<ul>
<li><code>sentence-transformers</code> 기반 한국어 특화 임베딩</li>
<li>벡터 DB 튜닝 (Top-k, filter, re-ranking 조정)</li>
<li>지원 검색 알고리즘, 오픈소스 유무와 사용 난이도 측면에서 가장 활용 가능성이 높은 <strong><code>Qdrant</code></strong> 활용</li>
</ul>
</li>
</ul>



| Vector DB     | 오픈소스 유무 | 지원언어              | 지원 검색 알고리즘      | 특징 |
|---------------|---------------|------------------------|--------------------------|------|
| Weaviate      | O             | Python, Go, Java, JS   | Custom HNSW              | - NLU 기능 내장으로 시맨틱 검색 강화<br>- GraphQL API 지원으로 간편한 질의 |
| Milvus        | O             | Python, Go, C++        | ANN, HNSW, ANNOY         | - 대규모 데이터셋에 강점<br>- 다양한 인덱싱 알고리즘 지원<br>- 높은 확장성 및 검색 성능<br>- K8s 기반 클라우드 배포 |
| Vespa         | O             | Java, C++              | HNSW (graph)             | - 실시간 인덱싱 및 데이터 처리 최적화<br>- 자동화된 스케일링<br>- 복잡한 설정과 관리<br>- 대규모 프로젝트 지향 |
| Vald          | O             | Go                     | NGT                      | - K8s 기반 자동화 운영<br>- 설정 및 운영 복잡 |
| Chroma        | O             | Python, JS             | HNSW                     | - 사용법 간단<br>- HTTP, 디스크, 인메모리 방식 선택 가능 |
| Qdrant        | O             | Rust                   | HNSW (graph)             | - JSON 기반 REST API 제공<br>- 직관적인 필터링 기능<br>- 빠른 검색 속도와 낮은 사용 난이도 |
| Pinecone      | X             | -                      | 복합 (비공개)            | - 클라우드 기반 (비용 발생)<br>- 자동 스케일링<br>- 간단한 API 사용<br>- 낮은 사용 난이도 |
| ElasticSearch | X             | -                      | Neural hashing, Hamming | - 커뮤니티 기반 사용<br>- 벡터 검색 기능은 제한적<br>- 설정 및 관리 복잡 |

<hr>
<h3><strong>5. 동시 요청 처리 최적화 (Concurrency Optimization)</strong></h3>
<ul>
<li><strong>목적</strong>: 다중 사용자 요청 대응 (목표 100명)</li>
<li><strong>기법</strong>:
<ul>
<li>요청 큐 시스템 구성 (asyncio queue or Redis + Celery)</li>
<li>추론 서버 분리 (<code>FastAPI</code> ↔ <code>LLM 추론 서버</code>)</li>
</ul>
</li>
<li><strong>사용 도구</strong>:
<ul>
<li><code>FastAPI</code>, <code>uvicorn</code>, <code>asyncio</code></li>
<li><code>Redis</code>, <code>Celery</code>, <code>RabbitMQ</code> (선택적)</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>6. 결과 캐싱 및 중복 응답 방지 (Latency Optimization)</strong></h3>
<ul>
<li><strong>목적</strong>: 동일 요청 반복 시 빠른 응답 제공</li>
<li><strong>기법</strong>:
<ul>
<li>Redis 기반 TTL 캐싱</li>
<li>TIL 텍스트 + 난이도 기준 해시 키 생성 후 결과 저장</li>
</ul>
</li>
<li><strong>사용 도구</strong>:
<ul>
<li><code>Redis</code>, <code>aioredis</code></li>
<li>SHA256 해시 함수 (TIL + 난이도 기반)</li>
</ul>
</li>
</ul>
<h2>병목 포인트</h2>

요소 | 설명 | 해결책
-- | -- | --
모델 처리 속도 | phi4-mini는 추론 속도가 빠른 편 (4~5초 내외) | 다중 사용자 동시 요청 시 대응 필요
GPU 자원 | L4 기준, 동시 요청 1~2개 수준 | 단일 GPU로는 동시 처리 어려움
RAG 검색 속도 | Qdrant 등 벡터 DB는 빠름 | 적절히 튜닝 시 안정적
서버 구조 | 요청 대기, 큐, 병렬성 구조 필요 | 이 부분이 핵심


<!-- notionvc: 5fe4499b-2a48-44b4-9dec-fb5a1f92499b -->
