### GCP VM 구성

- CPU: Intel Cascade Lake(vCPU 8개, 메모리 32GB)
    - GPU: NVIDIA L4 1ea
    - 부팅 디스크 용량: 200GB(Ubuntu 20.04 x86/64)

- CPU: Intel Cascade Lake(vCPU 8개, 메모리 32GB)
    - GPU: NVIDIA L4 1ea
    - 부팅 디스크 용량: 200GB(Ubuntu 20.04 x86/64)
    
    ### Model Inference 예시
    
    ```python
    from vllm import LLM, SamplingParams
    import torch
    from pydantic import BaseModel
    
    # 모델 위치
    gemma_3_4b_it =  "/home/mmm060400/KTB/models/gemma-3-4b-it/"
    # 모델 구동
    # 1. 모델 디렉토리 경로
    # 2. GPU 메모리 최대 사용치
    # 3. GPU 병렬 개수
    # llm = LLM(model=model_path, gpu_memory_utilization=0.95, tensor_parallel_size=1, max_model_len = 2048)
    llm = LLM(model=gemma_3_4b_it, 
              gpu_memory_utilization=0.95, 
              dtype= torch.bfloat16,
              tensor_parallel_size=1, 
              max_model_len = 2048,)
             
              
              
    from datetime import datetime
    now = datetime.now().strftime('%Y-%m-%d')
    lang = dataset["train"][0]["lang"] # github code review 데이터
    
    prompt = f"""
        You are an assistant that writes structured, markdown-based technical "Today I Learned (TIL)" reports in Korean, based on Git code diffs. 
    
        Your output must be a **valid JSON** object with three fields:
    
        1. "title": A concise and formal Korean title for the TIL. It should summarize the core change and include the date in the format: "{now} TIL - ...".
        2. "keywords": A list of 1 to 3 key Korean technical concepts (strings) that summarize the content. Each keyword should be one or two words, and relevant to the code changes.
        3. "content": The full TIL content in **Korean**, using Markdown. Structure it like this:
        - Start with a Markdown title: `# {now} TIL`
        - Summarize the key changes in a **formal, bullet-point list**
        - Include relevant **code snippets** below each point
        - End each bullet point with a period.
        - Optionally, explain maintainability, accessibility, or scalability concerns if they are relevant.
    
        Constraints:
        - Only return a JSON object with "title", "keywords", and "content" fields.
        - Do not include any extra commentary or preamble.
        - Avoid phrases like "I learned", "I think", or any personal expressions.
    
        Now, please generate the response based on the following code diff:
    
        {full_diff}
        """
    ```
    
    ### 모델 GPU VRAM 사용량 측정
    
    - Nvidia driver 설치 후 `watch -n 1 nvidia-smi` 로 초단위 GPU 사용량 모니터링 수행
    
<img width="658" alt="Image" src="https://github.com/user-attachments/assets/26ae894d-d074-4606-bc29-00f52d414671" />

    
## 모델 성능 지표 기준

| 항목 | `phi-4-mini-instruct` | `gemma-3-4b-it` |
| --- | --- | --- |
| 프롬프트 형식 | 자연어 그대로 OK | [INST]...[/INST] 필요 |
| 모델 크기 | 작음 (경량화) | 4.3B – 크고 메모리 많이 먹음 |
| 추론 안정성 | 높음 (에러 적음) | float16 / bf16 충돌 있음 |
| 사용 조건 | 누구나 접근 가능 | Hugging Face 로그인 + 토큰 필요 |
| Colab 안정성 | 매우 좋음 | 자주 CUDA assert 에러 발생 |

| 항목 | 측정 결과 | 비고 |
| --- | --- | --- |
| GPU 디바이스 | NVIDIA L4 | Colab Pro 환경 |
| 지연 시간 | 4.562초 | `max_new_tokens=100` 기준 |
| GPU 메모리 사용량 | 8747MiB / 23034MiB  | `nvidia-smi` 기준 |
| PyTorch 메모리 사용량 | 7324.76 MB  | `torch.cuda.memory_allocated` |
| PyTorch 메모리 캐기 | 8512.0MB | `torch.cuda.memory_reserved` |
| CPU 사용률 | 8.8% | 추론 시점 기준 |

### vLLM 추론 속도 비교

동일한 질문에 대한 응답 속도 비교

| 모델 | speed input | speed output | VRAM 용량 | 답변 품질 | 상용화 가능 여부 |
| --- | --- | --- | --- | --- | --- |
| Gemma3-4b-it | 0.86 toks/s | 28.21 toks/s |  21000MiB /  23034MiB | 대체적으로 만족스러움 | 가능 |
| Gemma3-1b-it | 1.19 toks/s | 86.35 toks/s |  21902MiB /  23034MiB | 매우 낮음, 거짓말 다수, 답변의 형식 부족 | 가능 |
| llama3.1-8b-it | 0.17 toks/s | 16.07 toks/s | 22084MiB /  23034MiB | 만족스러운 답변 수준 | 불가능 |
| deepseek-r1-qwen-7b | 1.07 toks/s | 16.78 toks/s | 22038MiB /  23034MiB | 답변 자체의 길이가 짧고, 성의가 없다. | 가능 |
| Qwen2.5-7b-it-1m | 0.30 toks/s | 16.85 toks/s |  22036MiB /  23034MiB | 답변의 성의가 없고, 형식화되지 않음 | 가능 |

### llama-cpp-python 추론 속도 비교

| 모델 | speed input | speed output | VRAM 용량 | 답변 품질 |
| --- | --- | --- | --- | --- |
| Qwen2.5-coder-3b.gguf(6bit) | 14.72 ms per token | 67.95 tokens per second |  5938MiB /  23034MiB  | 코드 해석은 우수함, Til 작성은 형식 아쉬움 |
| Gemma3-4b-it.gguf(6bit) | 14.76 ms per token | 67.76 tokens per second |  5970MiB /  23034MiB  | 답변 품질이 양자화 적용 후 아쉬움, 한국말 품질 저하 발생 |

### llama-cpp-python VS vLLm

| 항목 | vLLM | llama-cpp-python |
| --- | --- | --- |
| 제공 모델 | 허깅 페이스 기반 FP 16모델 | GGUF 형식의 경량화 모델 |
| 실행 환경 | CUDA 필수(GPU) | CPU or GPU |
| 지원 기능 | 비동기, KV 캐시, tensor parallel | 로우 레벨 추론, 다양한 양자화 모델 활용 가능 |
| 특징 | 대규모 LLM 서버 추론 최적화 | 초경량 추론 최적화 |
| 장점 | - GPU 활용 최적화 (`PagedAttention`)
- 다중 요청 처리에 강함
- `vllm.serve()`로 OpenAI 호환 API 제공
- FastAPI 연동 쉬움 | - 매우 낮은 메모리 점유 (INT4, INT8 등)
- CPU 기반도 가능- 가벼운 모델(1~4B) 추론 속도 빠름
- embedding/streaming 기능 포함 |
| 단점 | - VRAM 많이 사용
- 모델 로딩 시간 길고 무겁다
- 작은 GPU에서는 부적합 | - 모델 출력 품질 낮을 수 있음
- GPU 병렬처리/서버 성능 최적화 부족
- 대규모 트래픽 대응 어려움 |
| FastAPI 연동 | `vllm.serve()` 후 FastAPI proxy 구성 | `llama_cpp.server` 내장 or FastAPI 커스텀 구현 |

### vLLM 기반 LLM 서빙 최적화 계획

현재 구현 중인 TIL 자동 작성 시스템은 커밋 메시지, 코드 변경 이력 등 복합 정보를 입력으로 받아 마크다운 형식의 텍스트를 생성하는 생성형 태스크

이는 **연속된 대화 처리**, **다양한 길이의 입력 텍스트**, **스트리밍 응답**, 그리고 **짧은 대기시간**을 요구하는 서비스

이에 따라 모델 서빙은 다음 조건을 충족해야 함:

- 대기 시간(latency)이 짧음
- 동시 다중 요청 처리 처리량(throughput)이 높음
- 스트리밍 응답을 지원
- 일정 수준 이상의 응답 품질을 유지

이러한 요건에 가장 부합하는 추론 엔진은 **vLLM으로 결론**


---

## 적용예정 최적화 기법

### 1. **PagedAttention 기반의 시퀀스 병렬화 (Continuous Batching)**

- vLLM은 자체 구현한 `PagedAttention` 알고리즘을 통해, 서로 다른 길이의 시퀀스들을 하나의 배치(batch)로 처리
- 기존 Transformers 라이브러리에서는 padding이나 비효율적인 메모리 할당으로 인해 처리량 손실이 있었으나, vLLM은 GPU 메모리를 **가상 메모리 페이지처럼 슬라이스**해 최적화된 키/밸류 캐시 관리

➡️ **결과**: 대기 시간 감소 + 처리량 증가 (especially for concurrent users)

---

### 2. **비동기 FastAPI 연동 (StreamingResponse + asyncio)**

- 사용자 요청은 FastAPI 기반 프론트 서버로 받아, 내부적으로 vLLM 서버(`OpenAI API 호환`)와 비동기적으로 통신하도록 구성
- `stream=True` 모드 사용 시 응답이 생성되는 대로 사용자에게 실시간으로 반환되며, 체감 응답속도가 향상

**결과**: 사용자 응답 체감속도 ↑, UX 향상

---

### 3. **L4 GPU 최적 메모리 분할 및 모델 선택**

- `google/gemma3-4b-it` 또는 `Mistral-7B-Instruct` 계열 모델을 기준으로, `fp16` 정밀도 및 `tensor_parallel_size=1`로 최적 설정
- VRAM 24GB를 기준으로 최대 batch size ≒ 8~12 요청까지 동시 처리 가능(예상)

➡️ **결과**: 자원 효율성을 유지하면서도 고품질 응답 생성 가능

---

### **4. RAG 효율 최적화 (Embedding + 유사도 검색 속도 개선)**

- **목적**: 유사 질문 검색 정확도와 속도 향상
- **기법**:
    - `sentence-transformers` 기반 한국어 특화 임베딩
    - 벡터 DB 튜닝 (Top-k, filter, re-ranking 조정)
    - 지원 검색 알고리즘, 오픈소스 유무와 사용 난이도 측면에서 가장 활용 가능성이 높은 **`Qdrant`** 활용

| Vector DB     | 오픈소스 유무 | 지원 언어             | 지원 검색 알고리즘          | 특징 |
|---------------|---------------|------------------------|------------------------------|------|
| **Weaviate**  | ✅            | Python, GO, Java, JS  | Custom HNSW                 | - NLU 기능 내장으로 텍스트 데이터에 대한 시맨틱 검색 강화<br>- GraphQL API 지원으로 간편한 질의 가능 |
| **Milvus**    | ✅            | Python, GO, C++       | ANN, HNSW, ANNOY            | - 대규모 데이터셋에 강점<br>- 다양한 인덱싱 알고리즘 지원<br>- 높은 확장성 및 검색 성능<br>- K8s를 통한 클라우드 배포 가능 |
| **Vespa**     | ✅            | Java, C++             | HNSW (graph)                | - 실시간 인덱싱과 데이터 처리에 최적화<br>- 자동화된 스케일링<br>- 설정과 관리 방법은 다소 복잡<br>- 대규모 프로젝트에 적합 |
| **Vald**      | ✅            | Go                    | NGT                         | - K8s 기반의 자동화된 운영 및 확장성<br>- 설정과 운영 복잡 |
| **Chroma**    | ✅            | Python, JS            | HNSW                        | - 사용법 간단<br>- HTTP, 디스크 저장 방식, 인메모리 방식 선택 가능 |
| **Qdrant**    | ✅            | Rust                  | HNSW (graph)                | - JSON 기반의 REST API<br>- 직관적인 필터링(조회 조건 설정)<br>- 사용 난이도 낮고 빠른 검색 |
| **Pinecone**  | ❌            | -                     | 복합 사용 (독점)            | - 클라우드 기반(비용 발생)<br>- 데이터 양에 따른 오토 스케일링 가능<br>- 간단한 API 사용법<br>- 사용 난이도 낮음 |
| **ElasticSearch** | ❌       | -                     | Neural Hashing / Hamming Distance | - 커뮤니티 활발<br>- 전문 Vector DB에 비해 벡터 검색 기능 제한<br>- 설정 및 관리 복잡 |

---

**🔧 사용 도구 예시**:
- `KoSimCSE`, `klue/roberta`, `bge-m3` 등 한국어 임베딩 모델 활용 가능


 ---

### **5. 동시 요청 처리 최적화 (Concurrency Optimization)**

- **목적**: 다중 사용자 요청 대응 (목표 100명)
- **기법**:
    - 요청 큐 시스템 구성 (asyncio queue or Redis + Celery)
    - 추론 서버 분리 (`FastAPI` ↔ `LLM 추론 서버`)
- **사용 도구**:
    - `FastAPI`, `uvicorn`, `asyncio`
    - `Redis`, `Celery`, `RabbitMQ` (선택적)

---

### **6. 결과 캐싱 및 중복 응답 방지 (Latency Optimization)**

- **목적**: 동일 요청 반복 시 빠른 응답 제공
- **기법**:
    - Redis 기반 TTL 캐싱
    - TIL 텍스트 + 난이도 기준 해시 키 생성 후 결과 저장
- **사용 도구**:
    - `Redis`, `aioredis`
    - SHA256 해시 함수 (TIL + 난이도 기반)

## 병목 포인트

| 요소 | 설명 | 해결책 |
| --- | --- | --- |
| **모델 처리 속도** | phi4-mini는 추론 속도가 빠른 편 (4~5초 내외) | 다중 사용자 동시 요청 시 대응 필요 |
| **GPU 자원** | L4 기준, 동시 요청 1~2개 수준 | 단일 GPU로는 동시 처리 어려움 |
| **RAG 검색 속도** | Qdrant 등 벡터 DB는 빠름 | 적절히 튜닝 시 안정적 |
| **서버 구조** | 요청 대기, 큐, 병렬성 구조 필요 | 이 부분이 핵심 |
