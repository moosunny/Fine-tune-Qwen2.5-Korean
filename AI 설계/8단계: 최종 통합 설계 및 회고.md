![Image](https://github.com/user-attachments/assets/e854f3ac-72b6-41c8-a3e1-e6f863e9e2af)

<h2>1. 전체 서비스 아키텍처 개요</h2>
<ul>
<li>사용자 요청에 따라 다양한 AI 기능을 제공하는 멀티 인스턴스 기반 구조 설계</li>
<li>Google Cloud Load Balancer를 통해 AI Instance Group 1, 2로 트래픽 분산 처리</li>
<li>각 그룹 내 FastAPI 기반 API 서버가 모델 추론 결과를 처리하고 응답</li>
<li><code>/til</code>, <code>/news</code>, <code>/interview</code> 기능별로 분리하여 요청 최적화 처리</li>
<li>LoRA Adaptor 기반 경량화 모델 활용, Vector DB 기반 RAG 검색 시스템 구축</li>
<li>경량화 모델과 대용량 모델을 목적에 따라 조합하여 성능과 비용 효율 동시 확보</li>
<li>Prometheus와 Grafana를 활용해 인프라 상태를 실시간 모니터링</li>
</ul>
<hr>
<h2>2. 컴포넌트 도입 이유 및 효과</h2>

단계 | 주요 변경사항 | 성능/품질 향상 요약
-- | -- | --
1단계: API 설계 | FastAPI 기반 기능별 엔드포인트 분리 | API 응답속도 개선, 유지보수성 강화
2단계: 모델 선정 | Gemma-12B-GPTQ, Gemma-3B, Mistral, Tiny LLaMA 모델 선정 | 추론 속도와 메모리 최적화, 사용 목적에 따른 모델 적합성 확보
3단계: 모델 아키텍처 모듈화 | LLM과 LoRA Adapter 모듈 분리 | 업데이트 유연성 및 긴급 롤백 가능성 확보
4단계: LangChain 기반 워크플로우 구성 | Retriever, Prompt Generator, LLM Call 체계화 | 복잡한 프로세스의 확장성과 디버깅 용이성 향상
5단계: RAG 구축 | Qdrant 기반 벡터 검색 시스템 구축 | 문서 검색 정확도 향상, 응답 품질 강화
7단계: 인프라 이중화 및 모니터링 구축 | AI Instance Group 이중화, Prometheus+Grafana 적용 | 장애 무중단 대응, 리소스 병목 사전 감지 가능


<!-- notionvc: 5ba93ae3-9dfc-4088-b939-5a5d362c06c4 -->

---

<h2>3. 서비스 구성 요소 간 연동 흐름도</h2>
<ul>
<li>사용자(Client)는 Google Cloud Load Balancer를 통해 요청을 전송함</li>
<li>Load Balancer는 요청 URL을 기준으로 AI Instance Group 1 또는 2로 트래픽을 분산 처리함</li>
<li>각 AI Instance Group은 FastAPI 기반의 API 서버가 요청을 수신하고, 요청 경로에 따라 적절한 프로세스를 분기함
<ul>
<li><code>/til</code> 요청
<ul>
<li>FastAPI 서버가 요청 수신</li>
<li>Vector DB(Qdrant)로부터 관련 문서 검색</li>
<li>검색된 컨텍스트와 함께 LoRA Adapter가 적용된 Gemma-12B-GPTQ 모델로 TIL 초안 생성</li>
<li>생성된 결과를 API 응답으로 반환</li>
</ul>
</li>
<li><code>/news</code> 요청
<ul>
<li>FastAPI 서버가 요청 수신</li>
<li>추가적인 검색 없이 경량화된 Gemma-3B 모델을 바로 호출하여 뉴스 요약 생성</li>
<li>생성된 뉴스 요약 결과를 API 응답으로 반환</li>
</ul>
</li>
<li><code>/interview</code> 요청
<ul>
<li>FastAPI 서버가 요청 수신</li>
<li>Vector DB(Qdrant)를 통해 관련 데이터 검색</li>
<li>검색된 데이터 기반으로 Mistral 또는 Tiny LLaMA 모델이 면접 질문을 생성</li>
<li>생성된 질문을 API 응답으로 반환</li>
</ul>
</li>
</ul>
</li>
<li>모든 FastAPI 서버는 요청-응답 처리 과정에서 vLLM 엔진을 통해 모델 추론 최적화를 수행함</li>
<li>모든 인프라 자원(GPU, 인스턴스, 네트워크)은 Prometheus Exporter를 통해 메트릭을 수집함</li>
<li>수집된 메트릭은 Grafana 대시보드에 실시간으로 시각화되어 운영자가 상태를 모니터링함</li>
<li>향후 장애나 트래픽 이상 발생 시, Prometheus Alertmanager를 통해 Slack 또는 Email 알림이 추가될 예정임</li>
</ul>
<hr>
<h2>4. 전체 아키텍처 구성 평가</h2>
<ul>
<li>기능별 분리로 요청별 최적 경로 확보, 리소스 낭비 최소화</li>
<li>인스턴스 그룹 이중화와 Load Balancer 조합으로 가용성 강화</li>
<li>FastAPI와 vLLM 조합을 통한 대규모 요청 수용 및 추론 속도 향상</li>
<li>Vector DB를 통한 문맥 기반 정확한 검색 및 답변 품질 향상</li>
<li>Prometheus 기반 실시간 모니터링을 통해 장애 조기 탐지 및 대응 가능</li>
<li>결과적으로 확장성, 안정성, 성능을 모두 고려한 균형 잡힌 아키텍처 구성 달성</li>
</ul>
<hr>
<h2>5. 단계별 설계 적용 결과 요약</h2>
<ul>
<li><strong>1단계: API 설계</strong>
<ul>
<li>단일 서버 구조에서 기능별 엔드포인트(<code>/til</code>, <code>/news</code>, <code>/interview</code>)를 명확히 분리함</li>
<li>FastAPI를 적용하여 비동기 처리를 통해 요청 처리 속도를 개선함</li>
<li>OpenAPI 기반 명세 자동화를 통해 개발과 테스트 편의성을 확보함</li>
</ul>
</li>
<li><strong>2단계: 모델 선정</strong>
<ul>
<li>서비스 목적별로 최적화된 모델(Gemma-12B-GPTQ, Gemma-3B, Mistral, Tiny LLaMA) 구성</li>
<li>추론 성능과 리소스 효율성 균형을 고려하여 모델 선택을 최적화함</li>
<li>다양한 요청 부하에 대응할 수 있는 경량-대용량 모델 조합을 적용함</li>
</ul>
</li>
<li><strong>3단계: 모델 아키텍처 모듈화</strong>
<ul>
<li>LLM 본체와 LoRA Adapter를 완전히 분리하여 모듈화함</li>
<li>LoRA Adapter 교체만으로 사용자별 맞춤형 모델 튜닝이 가능하도록 개선함</li>
<li>모델 업데이트와 롤백을 다운타임 없이 처리할 수 있는 유연성을 확보함</li>
</ul>
</li>
<li><strong>4단계: LangChain 기반 워크플로우 설계</strong>
<ul>
<li>Retriever → Prompt Generator → LLM Call로 구성된 체계적인 체인 설계를 적용함</li>
<li>복잡한 RAG + 생성 프로세스를 명확히 구조화하여 확장성과 디버깅 용이성을 강화함</li>
<li>각 프로세스를 독립적으로 관리할 수 있도록 책임 범위를 명확히 분리함</li>
</ul>
</li>
<li><strong>5단계: RAG 구축 및 활용</strong>
<ul>
<li>Qdrant 기반 Vector DB를 적용하여 대규모 문서 검색 체계를 구축함</li>
<li>단순 생성이 아닌 문맥 기반 생성(RAG)으로 정보 신뢰성을 강화함</li>
<li>검색 정확도 향상을 위해 토큰 최적화, 메타데이터 필터링 전략을 추가 적용함</li>
</ul>
</li>
<li><strong>7단계: 인프라 이중화 및 모니터링 설계</strong>
<ul>
<li>Google Cloud Load Balancer를 통한 다중 인스턴스 부하 분산 구조를 완성함</li>
<li>Prometheus 기반 메트릭 수집과 Grafana 대시보드 시각화 시스템을 구축함</li>
<li>장애 발생 시 빠른 탐지 및 대응이 가능하도록 운영 안정성을 확보함</li>
</ul>
</li>
</ul>
<hr>
<h2>6. 향후 개선 방안</h2>
<ul>
<li><strong>자동 스케일링 도입</strong>
<ul>
<li>Kubernetes HPA 또는 GCP Autoscaler 적용 계획</li>
<li>트래픽 급증 시 자동 인스턴스 확장 및 비용 최적화 기대</li>
</ul>
</li>
<li><strong>응답 캐싱 전략 적용</strong>
<ul>
<li>Redis 기반 질의 결과 캐싱 시스템 구축</li>
<li>반복 요청 처리 시간 단축 및 서버 부하 분산 효과 기대</li>
</ul>
</li>
<li><strong>RAG 검색 시스템 고도화</strong>
<ul>
<li>Vector DB 인덱싱 최적화 및 메타데이터 기반 검색 강화</li>
<li>검색 정확도 및 응답 품질 추가 향상 목표</li>
</ul>
</li>
<li><strong>모니터링 알림 시스템 구축</strong>
<ul>
<li>Prometheus Alertmanager를 통한 Slack, 이메일 연동 알림 시스템 구축</li>
<li>장애 탐지 시 빠른 실시간 대응 체계 확립 계획</li>
</ul>
</li>
</ul>
<hr>
<h2>7. 설계 단계별 회고</h2>
<ul>
<li>다양한 기능(<code>/til</code>, <code>/news</code>, <code>/interview</code>)을 분리하여 독립적으로 운영할 수 있도록 설계했으며, 실제 서비스 시작 시 성능과 유지보수성 향상 효과를 기대</li>
<li>Instance Group 이중화와 Load Balancer 도입을 통해 특정 서비스 장애가 전체 서비스 중단으로 이어지지 않도록 방지할 계획</li>
<li>FastAPI + vLLM 조합을 통해 모델 추론과 API 서버 처리 속도를 최적화하여 고부하 환경에서도 안정적인 운영 가능</li>
<li>Vector DB 기반 RAG 시스템을 적용하여 생성 품질과 정보 신뢰성 향상 효과를 얻을 것으로 예상</li>
<li>Prometheus, Grafana를 활용한 모니터링 시스템 구축으로 서비스 운영 중 이상 징후를 빠르게 탐지하고 대응</li>
</ul>

설계 단계 | 회고 및 평가
-- | --
1단계: API 설계 | 기능별 API 분리 설계를 통해 실제 배포 시 응답 성능 향상과 코드 관리 효율성 개선
2단계: 모델 선정 | 대용량 모델과 경량 모델을 목적별로 선택하여 리소스 소모 최소화와 품질 유지 효과를 기대하며, 다양한 부하에 대응할 유연성도 확보
3단계: 모델 아키텍처 모듈화 | LLM과 LoRA Adapter를 모듈화하여 모델 업데이트 및 롤백 시 빠르고 안정적인 처리
4단계: LangChain 워크플로우 설계 | 체계적인 체인 구성을 통해 기능 추가 및 디버깅 용이성을 확보할 것으로 기대하며, 각 프로세스 독립성 강화를 통해 서비스 확장성에 대비할 예정
5단계: RAG 구축 및 활용 | Qdrant 기반 검색 시스템을 통해 문맥 기반 생성 품질 향상과 사용자 신뢰도 증가
7단계: 인프라 이중화 및 모니터링 설계 | Prometheus + Grafana 기반 모니터링 구축으로 운영 안정성과 장애 대응 속도 향상을 기대하며, 향후 Alertmanager 연동을 통한 알림 체계 강화 예정


<!-- notionvc: 5b7b6057-33eb-4d04-9520-d4c639a64aed -->
