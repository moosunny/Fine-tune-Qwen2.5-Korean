# 작성 일자: 25.04.24

## 1차 AI 서비스 인프라 아키텍처(Til 생성 API)

- GCP VM 기반 LLM API 아키텍처(FastAPI + vLLM + L4 GPU)
- Gemma3-12b-it-gptq 모델은 L4 GPU 환경에서 동시 30개의 요청을 수용 가능
- VM 2개를 생성하여 2배 동시 요청 Capacity 확보를 위해 GCP LB(Google Cloud Load Balancer) 도입
- Backend Service를 사용하여 GPU, CPU, 처리 요청 개수 기반으로 트래픽 제어 수행

<img width="672" alt="image" src="https://github.com/user-attachments/assets/e2bd2c3d-e7a7-4e62-97d8-22d0558cbb96" />


### 구성 요소 설명

| 구성 요소 | 역할 | 설명 |
| --- | --- | --- |
| **llm-vm-1, llm-vm-2** | L4 GPU 탑재 VM | FastAPI + vLLM 모델 서버 (Docker 컨테이너로 구동) |
| **Docker Image** | API 서버 코드 및 모델 포함 | Gemma3-12b-it-gptq |
| **GCP Load Balancer** | 외부 요청 분산 | 트래픽을 CPU 여유가 많은 VM으로 전달 |
| **Backend Service** | 트래픽 제어 | GPU, CPU 사용률, 처리 요청 개수 기준 분산 설정 (balancing-mode: UTILIZATION) |
| **Health Check** | VM 상태 확인 | `/healthz` 응답 기준으로 정상 VM만 유지 |
| **Monitoring** | 리소스 상태 추적 |`GCP Cloud Monitoring`은  비용 부과,`Prometheus`는 무료로 사용 가능 |

## 🛠️ 단계별 구성 절차 요약

### 1. L4 GPU VM 생성 (2대 이상)

- 모델 API가 포함된 Docker 이미지 실행
- 외부 포트 번호 지정하여 FastAPI 서버 구동
- `/healthz` 엔드포인트 제공
- LLM 추론 코드 및 컨테이너화
    
    
    | 항목 | 내용 |
    | --- | --- |
    | 추론 엔진 | [vLLM](https://github.com/vllm-project/vllm), [Triton Inference Server](https://github.com/triton-inference-server/server) |
    | 모델 | `gemma-12b-it-gptq` |
    | Dockerfile | 추론 코드와 모델 weight 포함, `multi-stage build` 방식으로 경량화 |
    | 주요 구성 | FastAPI 서버 , vLLM 기반 모델 로딩 |

### ✅ 2. Unmanaged Instance Group 생성

- VM들을 하나의 그룹으로 묶기

### ✅ 3. Backend Service 생성

- Instance Group을 대상(Target)으로 연결
- 트래픽 분산 정책: `UTILIZATION` + `max_utilization=0.8` 설정

### ✅ 4. GCP Load Balancer 구성

- `HTTP(S) Load Balancer` 생성
- `URL Map` 구성 후 Target Proxy 연결
- 포워딩 룰로 외부 요청 수신 (`:80`, 선택 시 `:443` SSL)

### ✅ 5. 도메인 + HTTPS 구성

- Cloud DNS로 도메인 연결
- GCP-managed SSL 인증서 자동 적용

### ✅  6. 모니터링 대상 지표 및 수집/시각화

| 항목 | 설명 | 수집 방식 | 시각화 도구 |
| --- | --- | --- | --- |
| **응답 시간 (Latency)** | LLM 1회 호출당 지연 시간 | Triton Metrics API + Prometheus | Grafana |
| **Throughput (TPS)** | 초당 처리 가능한 요청 수 | Prometheus scrape | Grafana |
| **GPU 사용량** | GPU Utilization, VRAM | NVIDIA DCGM Exporter | Grafana |
| **에러율** | 실패 요청 비율 (5xx, timeout 등) | FastAPI 미들웨어 + ELK | Grafana |

### 1차 서비스 인프라 아키텍처 도입으로 인해 얻을 수 있는 효과

| 항목 | 설명 |
| --- | --- |
| ✅ K8s 없이 간단 구성 | Docker + VM 기반이라 학습 볼륨 부담 적음 |
| ✅ GPU VM 직접 제어 가능 | vLLM, 모델 저장 위치, Docker run 옵션 자유 |
| ✅ GCP LB 기반 자동 분산 | CPU 사용률 기반으로 자동 분산 가능 |
| ✅ 도메인/HTTPS 구성도 가능 | Cloud DNS + SSL 인증서로 쉽게 구성 |
| ✅ 점진적 확장 가능 | VM 추가 후 Instance Group에만 넣으면 끝 |

---

## 2차 AI 서비스 인프라 아키텍처

- 고성능 LLM(Language Model) 기반 API 추론 서비스를 **클라우드(GKE)** 환경에서 안정적이고 유연하게 운영
- **FastAPI + vLLM 조합**을 사용하여 빠르고 경량화된 API 응답 제공
- 트래픽 증가에 유연하게 대응할 수 있는 **자동 확장(Auto-scaling)** 구조 도입
- **GPU 자원 활용 최적화**, **실시간 모니터링**, 보안 연결(HTTPS)까지 포함한 **생산환경 수준 인프라 구축**

<img width="670" alt="image" src="https://github.com/user-attachments/assets/0ce3fa37-a406-4528-8094-d8af5f27ec9f" />

<h2>구성 요소 설명</h2>

구성 요소 | 역할
-- | --
User/Client(API) | 서비스 사용자. HTTP 기반 API 요청을 전송
Google Cloud Load Balancer | 전 세계 유저 트래픽을 GKE 클러스터로 분산
GKE Ingress Controller | URL 기반 트래픽 라우팅 및 TLS 종단 처리
GPU NodePool | NVIDIA L4 GPU가 할당된 GKE 전용 노드 풀
LLM Pods (FastAPI + vLLM) | LLM 추론 엔진. FastAPI로 요청 처리, vLLM으로 모델 추론 수행
L4 GPU | 각 Pod에 할당된 추론 전용 GPU (NVIDIA L4)
Prometheus | 각 Pod/GPU에서 수집된 메트릭을 저장 및 시계열 분석
Grafana Dashboard | 실시간 서비스 모니터링 및 대시보드 시각화 구성 (GPU, latency, RPS 등)


<h2>🛠️ 단계별 구성 절차 요약</h2>
<h3>✅ 1. GKE 클러스터 및 GPU 노드풀 준비</h3>
<ul>
<li><strong>목적</strong>: L4 GPU를 활용할 수 있는 GKE 환경 구성</li>
<li>수행 단계:
<ul>
<li>GKE 클러스터 생성</li>
<li>NVIDIA L4 GPU 노드풀 추가</li>
<li>NVIDIA 드라이버 자동 설치용 DaemonSet 배포</li>
</ul>
</li>
</ul>
<h3>✅ 2. FastAPI + vLLM Docker 이미지 생성(1차 배포 Dockerfile 활용)</h3>
<ul>
<li>목적: 추론 서버 애플리케이션 컨테이너로 패키징</li>
<li>수행 단계
<ol>
<li>Dockerfile 작성</li>
<li>Dockerfile 이미지 빌드</li>
<li>GCP Artifact Registry에 이미지 push</li>
</ol>
</li>
</ul>
<h3>✅ 3. Helm Chart로 애플리케이션 배포</h3>
<ul>
<li><strong>목적</strong>: FastAPI + vLLM 서버를 GKE에 유연하게 배포</li>
<li>수행 단계:
<ol>
<li>Helm Chart 구성 준비</li>
<li>모델 서버 배포</li>
<li>GPU 전용 노드에만 배포되도록 설정(<code>nodeSelector</code>, <code>tolerations</code>)</li>
</ol>
</li>
</ul>
<h3>✅ 4. Ingress 및 도메인/SSL 설정</h3>
<ul>
<li>목적: 외부에서 안전하게 API 접근 가능하게 구성</li>
<li>수행 단계:
<ol>
<li>Ingress 리소스 배포</li>
<li>ManagedCertificate 리소스 작성(도메인 SSL 자동 적용)</li>
<li>Cloud DNS에 A 레코드 등록 → GCLB IP 매핑</li>
</ol>
</li>
</ul>
<h3>✅ 5. 모니터링 시스템 구성</h3>
<ul>
<li><strong>목적</strong>: 추론 서비스 상태 및 성능 모니터링</li>
<li>수행 단계:
<ol>
<li>
<p>Prometheus 설치 (Helm 사용 예정)</p>
</li>
<li>
<p>각 노드에 Exporter 배포</p>
<ul>
<li><code>node-exporter</code>, <code>kube-state-metrics</code>, <code>dcgm-exporter</code></li>
</ul>
</li>
<li>
<p>Grafana에 대시보드 구성</p>
<p>기본 대시보드 + NVIDIA GPU 대시보드 불러오기</p>
</li>
</ol>
</li>
</ul>
<h3>✅ 6. 수평 확장 설정(HPA + GPU AutoProvisioning)</h3>
<ul>
<li>목적: 트래픽에 따라 자동 확장되는 구조 구성</li>
<li>수행 단계:
<ol>
<li>HorizontalPodAutoScaler 설정</li>
<li>GPU 노드 자동 생성 옵션 설정(Auto Provisioning)</li>
</ol>
</li>
</ul>
<h3>2차 서비스 인프라 아키텍처 도입으로 인해 얻을 수 있는 효과</h3>
<ul>
<li><strong>vLLM 기반 고성능 LLM 추론 API 자동화 배포</strong></li>
<li><strong>GCS에서 모델 동적 로딩 → 경량 이미지 구성</strong></li>
<li><strong>Ingress + HTTPS + 도메인 완전 자동화</strong></li>
<li><strong>Prometheus + Grafana 기반 시각화 및 경고</strong></li>
<li><strong>트래픽 변화에 따라 Pod 수 + GPU 노드 자동 확장</strong></li>
</ul>
<!-- notionvc: c9641eff-5b42-4c13-a137-bfb46c104587 -->
