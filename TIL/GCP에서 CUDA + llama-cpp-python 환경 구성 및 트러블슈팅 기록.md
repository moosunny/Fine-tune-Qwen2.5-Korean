### ğŸ—“ï¸ ë‚ ì§œ: 25.04.10

### ëª©í‘œ: GCP VM ìƒì„± ë° CUDA toolkit, NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ + llama-cpp-Python êµ¬ë™

## 1. GCP VM êµ¬ì„±

- CPU: Intel Cascade Lake(vCPU 8ê°œ, ë©”ëª¨ë¦¬ 32GB)
- GPU: NVIDIA L4 1ea
- ë¶€íŒ… ë””ìŠ¤í¬ ìš©ëŸ‰: 200GB(Ubuntu 20.04 x86/64)

## 2. NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜

- Ubuntu 20.04 ê¸°ë°˜ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì¬ì„¤ì¹˜ (ê¸°ì¡´ Debianì€ backports ë¬¸ì œ ë“±ìœ¼ë¡œ VM ì‚­ì œ í›„ ì¬ìƒì„±)
- `nvidia-smi` ëª…ë ¹ì´ ì‘ë™í•˜ì§€ ì•Šì•„ NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ ì§„í–‰
- GCP ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ **ê¶Œì¥ ë“œë¼ì´ë²„ ë²„ì „ (550.144.03)** ìˆ˜ë™ ì„¤ì¹˜
- ì´í›„ `nvidia-smi` ì‘ë™ í™•ì¸ â†’ L4 GPU, CUDA 12.4 ì •ìƒ ì¶œë ¥

```bash
sudo apt install nvidia-driver-550
nvidia-smi
```

## 3. CUDA Toolkit 12.4 ì„¤ì¹˜

- `nvidia-smi`  ì‹¤í–‰ í›„ CUDA ë²„ì „ í™•ì¸

![Image](https://github.com/user-attachments/assets/235141ba-4e16-4046-9c17-242ca503d1ed)

- NVIDIA ê³µì‹ ë ˆí¬ì§€í† ë¦¬ ì¶”ê°€ í›„ CUDA 12.4 ì„¤ì¹˜ ì§„í–‰

```bash
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub
sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"
sudo apt update
sudo apt install cuda-toolkit-12-4

=> nvcc --version  # CUDA 12.4.131
```

## 4. `llama-cpp-python`  GPU ë²„ì „ ì„¤ì¹˜

### âŒ ë¬¸ì œ ë°œìƒ

- `CMAKE_ARGS="-DLLAMA_CUBLAS=ON"` í”Œë˜ê·¸ëŠ” **deprecated**
- ì´í›„ `GGML_CUDA=ON` ìœ¼ë¡œ ìˆ˜ì •í–ˆì§€ë§Œ ì—¬ì „íˆ CMake ë¹Œë“œ ì‹¤íŒ¨
- ì›ì¸: **Anacondaì˜ ë‚´ë¶€ ë§ì»¤(`ld`)ê°€ glibc ì‹œìŠ¤í…œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¸ì‹ ëª»í•¨**
    - ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ venvë¡œ ê°€ìƒí™˜ê²½ ì¬êµ¬ì„± ì§„í–‰

```bash
# llama-cpp-python Prebulit CUDA wheel ì„¤ì¹˜(ì„±ê³µ)
pip install llama-cpp-python==0.2.77 \
  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124 \
  --no-cache-dir
```

### Qwen2.5-coder-3b-q6, Gemma-3-4b-it-6q gguf ëª¨ë¸ ë‹¤ìš´(êµ¬ê¸€ ë“œë¼ì´ë¸Œ)

```bash
# gdownìœ¼ë¡œ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ gguf íŒŒì¼ ë‹¤ìš´ë¡œë“œ
# í˜•ì‹: https://drive.google.com/file/d/15uSQ5m_l0Ty3w1XBastRweh3ZKqmUbZ4/view?usp=drive_link
# Qwen2.5-coder-3b-Q6
!gdown gdown https://drive.google.com/uc?id=15uSQ5m_l0Ty3w1XBastRweh3ZKqmUbZ4 -O /home/mmm060400/KTB/models/qwen2.5-coder-3b-instruct-q6_k.gguf
# Gemma-3-4b-it-Q6
!gdown https://drive.google.com/uc?id=15uSQ5m_l0Ty3w1XBastRweh3ZKqmUbZ4 -O /home/mmm060400/KTB/models/gemma-3-4b-it-Q6_K.gguf
```

### Qwen2.5-coder-3b ëª¨ë¸ ë¡œë“œ ë° GPU ì§€ì› í™•ì¸

```python
from llama_cpp import Llama

code_llm = Llama(model_path="/home/mmm060400/KTB/models/qwen2.5-coder-3b-instruct-q6_k.gguf",
                n_ctx = 2048,
                 n_gpu_layers=100, # Qwen2.5-coder-3b ëª¨ë¸ì€ 37ê°œ layer êµ¬ì„±
                 verbose = True
                 )
```

<img width="412" alt="Image" src="https://github.com/user-attachments/assets/76bc376d-4844-4376-a13a-333d8c654504" />

| 36ê°œ ë°˜ë³µ ë ˆì´ì–´ë¥¼ GPUë¡œ ì´ë™ | íŠ¸ëœíŠ¸í¬ë¨¸ block ì „ì²´ë¥¼ CUDAì—ì„œ ê³„ì‚° |
| --- | --- |
| ì´ 37ê°œ ë ˆì´ì–´ GPU í• ë‹¹ ì„±ê³µ | ì „ì²´ ëª¨ë¸ì´ GPUì—ì„œ ë™ì‘ ì¤‘ |
| CUDA ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ì•½ 2.4GB | Q6_K 6bit ëª¨ë¸ì´ë¼ 10%ì˜ GPU ë©”ëª¨ë¦¬ ë§Œì„ ì‚¬ìš© |

## 4. ëª¨ë¸ ì¶”ë¡ ì„ í†µí•œ GPU ì„±ëŠ¥ ì¸¡ì •

- ì‹¤ì œ ì´ì „ CPUë§Œì„ í™œìš©í•œ ì¶”ë¡ ì— ë¹„í•´ 2ë°° ì´ìƒ ë¹ ë¥¸ ì†ë„ë¡œ ì¶”ë¡ ì„ ì§„í–‰í•˜ëŠ” ê²ƒì„ í™•ì¸í•¨

```python
# github commit data ê¸°ë°˜ ì½”ë“œ ë¦¬ë·°ë¥¼ Qwen2.5-coder-3b ëª¨ë¸ë¡œ ì¶”ë¡  ì§„í–‰
query = dataset["train"][0]["code"]

output = code_llm(f"""ì£¼ì–´ì§„ ì½”ë“œ ë³€ê²½ ë°ì´í„°ì— ëŒ€í•œ ì½”ë“œ ë¦¬ë·°ë¥¼ ë¶€íƒí•´,
                  @@ -0,0 +0,00 @@ ë¼ëŠ” ë¬¸êµ¬ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ì¤„ì— ì½”ë“œ ë³€ê²½ ì´ë ¥ì´ ìˆë‹¤ëŠ” ë‚´ìš©ì´ë¼ í•´ë‹¹ ë¶€ë¶„ì— ì§‘ì¤‘í•´ì„œ ì–´ë–¤ ì½”ë“œê°€ ë³€ê²½ì´ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•´ì¤˜
                  
                  ì§ˆë¬¸: {query},
                  ë‹µë³€: 
                  """ ,
                  max_tokens=1024, 
                  stream=True, 
                  stop = "<|endoftext|>")

for word in output:
  print(word["choices"][0]["text"], end = "", flush=True)
  
# GPU(L4) ê¸°ë°˜ ì¶”ë¡  ì„±ëŠ¥ ê²°ê³¼
llama_print_timings:        load time =     493.09 ms
llama_print_timings:      sample time =    2474.95 ms /  1024 runs   (    2.42 ms per token,   413.75 tokens per second)
llama_print_timings: prompt eval time =     601.22 ms /   769 tokens (    0.78 ms per token,  1279.06 tokens per second)
llama_print_timings:        eval time =   15054.69 ms /  1023 runs   (   14.72 ms per token,    67.95 tokens per second)
llama_print_timings:       total time =   20635.41 ms /  1792 tokens
```

## ğŸ’¡ ë°°ìš´ ì 

- Anaconda í™˜ê²½ì€ glibc ë§í¬ ë¬¸ì œë¡œ CUDA ì—°ë™ì— ì·¨ì•½í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, venv í™œìš©
- ìµœì‹  `llama-cpp-python`ì€ `GGML_CUDA=ON`ìœ¼ë¡œ ì„¤ì •í•´ì•¼ GPU ë°±ì—”ë“œ ì‚¬ìš© ê°€ëŠ¥
- GCPì—ì„œëŠ” L4 GPU ê¶Œì¥ ë“œë¼ì´ë²„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í™•ì¸í•˜ê³  ì ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”
