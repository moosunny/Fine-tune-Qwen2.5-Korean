{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "    token= \"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\",\n",
    "    #add_to_git_credential=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import (\n",
    "    DataCollatorForCompletionOnlyLM,\n",
    "    setup_chat_format,\n",
    "    SFTTrainer\n",
    ")\n",
    "\n",
    "from peft import (AutoPeftModelForCausalLM,\n",
    "                 LoraConfig,\n",
    "                 PeftConfig)\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          Trainer,\n",
    "                          TrainingArguments,\n",
    "                          BitsAndBytesConfig,\n",
    "                          pipeline,\n",
    "                          StoppingCriteria,\n",
    "                          TextStreamer)\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    # load_in_8bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"google/gemma-3-1b-it\", device=\"cuda\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant. if there any comment about korea, you should answer in korean.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'You are a helpful assistant. if there any comment about korea, you should answer in korean.'}]},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text', 'text': 'í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': 'ë„¤, í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì— ëŒ€í•´ ì„¤ëª…í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n\\n1.  **ì´ìŠ¹ë§Œ (1948ë…„ ~ 1960ë…„)**: ëŒ€í•œë¯¼êµ­ ì´ˆëŒ€ ëŒ€í†µë ¹ìœ¼ë¡œ'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.bfloat16. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„¤, í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì— ëŒ€í•´ ì„¤ëª…í•´ ë“œë¦´ê²Œìš”! í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   **êµ°ìƒí˜¸(æ–‡ç¥¥, 1948å¹´ - 1952å¹´):** ëŒ€í•œë¯¼êµ­ ì´ˆëŒ€ ëŒ€í†µë ¹\n",
      "*   **ì´ìŠ¹ë§Œ(ææ‰¿æ™©, 1948å¹´ - 1960å¹´):** ëŒ€í•œë¯¼êµ­ ì´ˆëŒ€ ëŒ€í†µë ¹\n",
      "*   **ìœ¤ë³´ì„ (å°¹ä¼¯ì„ , 1960å¹´ - 1962å¹´):** ëŒ€í•œë¯¼êµ­ ì´ˆëŒ€ ëŒ€í†µë ¹\n",
      "*   **ì´íš¨ì„(æå­çŸ³, 1962å¹´ - 1963å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹ (êµ­ë¯¼ì£¼ê¶Œë‹¹ ëŒ€í‘œ)\n",
      "*   **ì „ëŒ€í†µë ¹ ì´í•œë„¤(æéŸ“èª NE) (1963å¹´ - 1966å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹ (êµ­ë¯¼ì£¼ê¶Œë‹¹ ëŒ€í‘œ)\n",
      "*   **ì´ë§Œë³µ(ææ˜ç¦, 1966å¹´ - 1972å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **ê¹€ì˜ì‚¼(é‡‘è‹± Sonn, 1972å¹´ - 1979å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **í•œì¸ì„­(éŸ“é€²æ”, 1979å¹´ - 1980å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **ê¹€ëŒ€ì¤‘(é‡‘æ­£ç¾©, 1980å¹´ - 2003å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **ë…¸ë¬´í˜„(ë…¸ë¬´í˜„, 2003å¹´ - 2008å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **ì´ëª…ë°•(ææ˜åš, 2008å¹´ - 2013å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **ë°•ê·¼í˜œ(ë°•ê·¼í˜œ, 2013å¹´ - 2017å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "*   **ë¬¸ì¬ì¸(æ–‡åœ¨å¯…, 2017å¹´ - 2022å¹´):** ëŒ€í•œë¯¼êµ­ ëŒ€í†µë ¹\n",
      "\n",
      "í˜¹ì‹œ íŠ¹ì • ëŒ€í†µë ¹ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹œê±°ë‚˜, ì—­ì‚¬ì ì¸ ë°°ê²½ì´ë‚˜ ì‚¬ê±´ì— ëŒ€í•´ ë” ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant. if there any comment about korea, you should answer in korean.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"í•œêµ­ì˜ ì—­ëŒ€ ëŒ€í†µë ¹ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device).to(torch.bfloat16)\n",
    "\n",
    "\n",
    "# ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³  generate ë¶€ë¶„ë§Œ ìˆ˜ì •\n",
    "with torch.inference_mode():\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=1024,\n",
    "        streamer=streamer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-31 12:32:52--  https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30560672 (29M) [application/octet-stream]\n",
      "Saving to: â€˜total_kor_multiturn_counsel_bot.jsonlâ€™\n",
      "\n",
      "total_kor_multiturn 100%[===================>]  29.14M  67.0MB/s    in 0.4s    \n",
      "\n",
      "2025-03-31 12:32:54 (67.0 MB/s) - â€˜total_kor_multiturn_counsel_bot.jsonlâ€™ saved [30560672/30560672]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/MrBananaHuman/CounselGPT/main/total_kor_multiturn_counsel_bot.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./total_kor_multiturn_counsel_bot.jsonl', \n",
    "          'r', \n",
    "          encoding='utf-8') as file:\n",
    "    original_jsonl_data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'ìƒë‹´ì‚¬', 'utterance': 'ì•ˆë…•í•˜ì„¸ìš”. ì‹¬ë¦¬ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì–´ë–¤ ê³ ë¯¼ì´ ìˆìœ¼ì‹ ê°€ìš”?'},\n",
       " {'speaker': 'ë‚´ë‹´ì', 'utterance': 'ìš”ì¦˜ ì§ì¥ì—ì„œ ë„ˆë¬´ í˜ë“¤ì–´ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬', 'utterance': 'ì •ë§ìš”? ì–´ë–¤ ì ì´ í˜ë“œì‹ ê°€ìš”? ì¢€ ë” ìì„¸íˆ ë§í•´ì£¼ì„¸ìš”.'},\n",
       " {'speaker': 'ë‚´ë‹´ì',\n",
       "  'utterance': 'ì¹œí•œ ë™ë£Œë„ ì—†ê³  ì¼ì´ ë„ˆë¬´ ë§ê³  ê³ ê°ì´ë‚˜ ë™ë£Œì—ê²Œ ë§¤ì¼ ë°˜ì‘í•˜ê³  ëŒ€ì²˜í•´ì•¼í•˜ë‹ˆê¹Œ ì ì  ì§€ì³ ê°€ë„¤ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬',\n",
       "  'utterance': 'ê·¸ëŸ¬ì…¨êµ°ìš”. ì§ì¥ìƒí™œì—ì„œ í•˜ë‚˜í•˜ë‚˜ ëŒ€ì‘í•˜ëŠ” ì¼ì€ ë§ì€ ì—ë„ˆì§€ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŸ¬í•œ ì—ë„ˆì§€ ì†Œëª¨ëŠ” ê¸‰ê²©íˆ í˜ë“¤ì–´ì§€ê²Œ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¼ìƒì— ì ì‘í•˜ë©° ì‹œê°„ì´ ì§€ë‚˜ë©´ ì ì  í˜ë“¤ì–´ì§ˆ ìˆ˜ ìˆì–´ìš”.'},\n",
       " {'speaker': 'ë‚´ë‹´ì', 'utterance': 'ì§‘ì— ê°€ë©´ ì§‘ì•ˆì¼ì„ í•˜ê³  ë‚˜ë©´ ë¬´ì–¸ê°€ë¥¼ í•´ì•¼í•˜ëŠ”ë° ê·¸ê²Œ ë„ˆë¬´ í˜ë“¤ì–´ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬',\n",
       "  'utterance': 'ì§‘ì—ì„œë„ ì¼ì„ í•˜ì‹œëŠ”êµ°ìš”. ê·¸ëŸ¬ì‹œë‹¤ë³´ë‹ˆ ì§‘ì—ì„œì˜ ì¼ë„ ì˜ë¬´ì ìœ¼ë¡œ ëŠê»´ì§€ëŠ” ê±° ê°™ì•„ìš”. ì´ëŸ¬í•œ ì˜ë¬´ê°ì— ì˜í•´ì„œ ë¶ˆì•ˆê°ê³¼ í˜ë“¤ì–´ì§ˆ ìˆ˜ ìˆì£ .'},\n",
       " {'speaker': 'ë‚´ë‹´ì', 'utterance': 'ì´ëŸ¬ë‹¤ ëª¸ì´ ì•„í”Œ ê²ƒ ê°™ì•„ìš”. ì´ê²Œ ê³„ì†ë˜ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬',\n",
       "  'utterance': 'ëª¸ì´ í˜ë“¤ì–´ì§€ëŠ” ê±´ ìì‹ ì´ ì§€ë‹ˆê³  ìˆëŠ” ì‹ í˜¸ì…ë‹ˆë‹¤. ì¦‰, ëª¸ê³¼ ë§ˆìŒì— ì‹ í˜¸ë¥¼ ì£¼ê³  ìˆëŠ” ê±°ì£ . í˜¹ì‹œ ì´ëŸ¬í•œ ì¦ìƒì´ ì§€ì†ë˜ì‹œë©´ ì£¼ë³€ì˜ ë‚´ìš©ì„ í†µí•´ ì£¼ë³€ì˜ ìƒí™©ì„ ì‚´í´ë³´ê³ , ë‹¤ì–‘í•œ ìì‹ ì˜ ì·¨ë¯¸ë¥¼ ë°œê²¬í•˜ê±°ë‚˜, íœ´ì‹ì„ í†µí•´ì„œ ì‰¬ëŠ” ê²ƒë„ ì¢‹ìŠµë‹ˆë‹¤. ë§Œì•½ì— ëª¸ì— ì´ìƒì„ ëŠë¼ì‹ ë‹¤ë©´ ë³‘ì›ì— ì°¾ì•„ê°€ì…”ì„œ ë‹¤ì–‘í•œ ê±´ê°•ìƒì˜ ë¬¸ì œë¥¼ ì˜ˆë°©í•  ìˆ˜ ìˆë„ë¡ ì¡°ì¹˜í•˜ì„¸ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬', 'utterance': 'ë‚´ë‹´ìë‹˜, ì–´ë–¤ ìƒê°ì´ ë“œì‹œë‚˜ìš”?'},\n",
       " {'speaker': 'ë‚´ë‹´ì', 'utterance': 'ìƒê°ì„ ì˜ ëª»í•´ì„œìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬',\n",
       "  'utterance': 'ê·¸ëŸ¬ì‹œë©´, ìš°ì„  ì´ëŸ¬í•œ ì¼ìƒì— ëŒ€í•´ì„œ ê³ ë¯¼í•´ë³´ì„¸ìš”. ë¨¸ë¦¬ë¥¼ ë¹„ìš°ê³  ì‰¬ì–´ë„ ì¢‹ê³ , ì§„ì§€í•˜ê²Œ ìì‹ ì˜ ì¼ìƒì„ ëŒì•„ë³´ë©´ì„œ ì–´ë–»ê²Œ í•˜ë©´ ì´ëŸ¬í•œ ê³ ë¯¼ì„ ì¤„ì¼ ìˆ˜ ìˆëŠ”ì§€ ìƒê°í•´ë³´ì„¸ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬', 'utterance': 'ì–´ë–¤ ìƒê°ì„ í•˜ì…¨ë‚˜ìš”?'},\n",
       " {'speaker': 'ë‚´ë‹´ì', 'utterance': 'ê°€ì¡±ì´ë‘ ì‹œê°„ì„ ë³´ë‚´ë©´ì„œ ì¦ê²ê²Œ ìƒí™œí•´ì•¼ê² ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆì–´ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬',\n",
       "  'utterance': 'ê·¸ë ‡êµ°ìš”. ê°€ì¡±ì´ë‚˜ ì¹œêµ¬ì™€ì˜ ì†Œí†µì€ ê·¸ë§Œí¼ì˜ ë§Œì¡±ê°ê³¼ í¸ì•ˆí•¨ì„ ê°€ì ¸ë‹¤ì¤„ ìˆ˜ ìˆì£ . ë‹¤ì–‘í•œ ì‹œê°„ê³¼ ê²½í—˜ì„ ë‚˜ëˆ„ë©´ì„œ ê·¸ ì‚¬ëŒë“¤ê³¼ ë” ê°€ê¹Œì›Œì§ˆ ìˆ˜ ìˆì„ ê±° ê°™ì•„ìš”.'},\n",
       " {'speaker': 'ìƒë‹´ì‚¬', 'utterance': 'ë” ë§ì”€í•˜ì‹¤ ë‚´ìš©ì´ ìˆìœ¼ì‹ ê°€ìš”?'},\n",
       " {'speaker': 'ë‚´ë‹´ì', 'utterance': 'ì—†ì–´ìš”. ê°ì‚¬í•©ë‹ˆë‹¤.'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_jsonl_data[5085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
